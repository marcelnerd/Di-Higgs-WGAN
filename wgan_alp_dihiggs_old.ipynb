{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "%run dihiggs_dataset.ipynb\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   # NOTE: I don't think this is used\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--n_epochs N_EPOCHS]\n",
      "                             [--batchSize BATCHSIZE]\n",
      "                             [--learningRate LEARNINGRATE] [--beta1 BETA1]\n",
      "                             [--beta2 BETA2]\n",
      "                             [--latentSpaceSize LATENTSPACESIZE]\n",
      "                             [--lrDecayRate LRDECAYRATE] [--width WIDTH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/mcnama20/.local/share/jupyter/runtime/kernel-94a4bc80-bdd1-4c69-a965-1fb39dc52a69.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--n_epochs\", type=int, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batchSize\", type=int, help=\"size of the batches\")\n",
    "parser.add_argument(\"--learningRate\", type=float, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--beta1\", type=float, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--beta2\", type=float, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--latentSpaceSize\", type=int, help=\"dimensionality of the latent space\")\n",
    "#parser.add_argument(\"--n_critic\", type=int, default=5, help=\"number of training steps for discriminator per iter\")\n",
    "parser.add_argument(\"--lrDecayRate\", type=float, help=\"Learning rate decay rate\")\n",
    "parser.add_argument(\"--width\", type=int, help=\"Width of network\")\n",
    "parser.add_argument(\"--depth\", type=int, help=\"Depth of network\")\n",
    "parser.add_argument(\"--activationFunction\", type=str, help=\"Activation function used\")\n",
    "parser.add_argument(\"--alpha\", type=float, help=\"Alpha value for LeakyRelu (if used)\")\n",
    "parser.add_argument(\"--batch_norm\", type=bool, help=\"True or false value for using batch normalization\")\n",
    "parser.add_argument(\"--configDir\", type=str, help=\"Directory to save data to\")\n",
    "\n",
    "opt = parser.parse_args()\n",
    "print(opt)\n",
    "\n",
    "class opt_old():   # Class used for optimizers in the future. Defines all variables and stuff needed.\n",
    "    n_epochs = 20000   # an epoch is the number of times it works through the entire training set\n",
    "    batch_size = 1000   # the training set is broken up into batches, \n",
    "                        # and the average loss is used from a given batch for back propagation\n",
    "    lr =  0.0002 # 0.001   # learning rate (how much to change based on error)\n",
    "    b1 = 0     # 0.9 # Used for Adam. Exponential decay rate for the first moment. \n",
    "    b2 = 0.9   # 0.999 # Used for Adam. Exponential decay rate for the second moment estimates (gradient squared)\n",
    "    #NOTE: The default epsilon for torch.optim.adam is 1e-8, so I will just leave it that way\n",
    "    \n",
    "    #n_cpu = 2   # not used rn\n",
    "    latent_dim = 100 #size of noise input to generator (latent space) \n",
    "    #img_size = 28\n",
    "    # channels = 1   # Only used for img_shape right below, and img_shape isn't needed\n",
    "    n_critic = 5   # The generator is trained after this many critic steps\n",
    "    #   clip_value = 0.01   # No other usages rn. \n",
    "    sample_interval = 400   # Determines when a to save the image(s?) generated\n",
    "    \n",
    "    Xi = 10;   # multiplier for recursively finding r_adversarial\n",
    "    \n",
    "    # Loss weight for alp penalty\n",
    "    lambda_alp = 100\n",
    "\n",
    "# img_shape = (opt.channels, opt.img_size, opt.img_size)   # Not used rn\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "sample_interval = 400\n",
    "n_critic = 5\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.configDir)\n",
    "except OSError:\n",
    "    logging.warning(\"Output folders already exist. May overwrite some output files.\")\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Create hidden layers. Apply normalization. Apply leaky relu. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()   \n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):   # This function creates the hidden layers\n",
    "            layers = [nn.Linear(in_feat, out_feat)]   # layer is a hidden layer. Takes input\n",
    "                                                      # (batch_size,in_feat) and give an output (batch_size,out_feat)\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))   # adds normalization to what Layers does to input and comes out in\n",
    "                                                               # size (batch_size,out_feat). I think this does bn1d(linear(input))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))   # inplace means just modify input, don't allocate more memory\n",
    "            return layers\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        stores layers and functions applied to layers\n",
    "        \"\"\"   \n",
    "        if os.path.exists(opt.configDir + \"/generatorModel.pt\"): \n",
    "            self.model = torch.load(opt.configDir + \"/generatorModel.pt\")\n",
    "        else:\n",
    "            self.model = nn.Sequential(   \n",
    "                *block(opt.latent_dim, 128, normalize=False),   # first layer\n",
    "                *block(128, 256),   # second layer\n",
    "                *block(256, 512),   # 3rd layer\n",
    "                *block(512, 1024),   # 4th layer\n",
    "                nn.Linear(1024, 25),   # final layer. Output is size 25\n",
    "                nn.Tanh()   # Using tanh for final output (why tanh vs leaky relu?)\n",
    "            )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        applies layers to input to get img\n",
    "        \"\"\"\n",
    "        img = self.model(z)   # applies model (layers and functions on layers) to z\n",
    "        #img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator/critic layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()   # Just uses the module constructor with name Discriminator \n",
    "\n",
    "        if os.path.exists(opt.configDir + \"/discriminatorModel.pt\"): \n",
    "            self.model = torch.load(opt.configDir + \"/discriminatorModel.pt\")\n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(25, 512),   # first layer\n",
    "                nn.LeakyReLU(0.2, inplace=True),   # apply leaky relu to layer\n",
    "                nn.Linear(512, 256),   # 2nd layer\n",
    "                nn.LeakyReLU(0.2, inplace=True),   # apply leaky relu to layer\n",
    "                nn.Linear(256, 1),   # Final layer to give output. Output is size 1 (validity score)\n",
    "                                     # NOTE: weird to end with comma\n",
    "            )\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        applies model to image and gives validity score\n",
    "        \"\"\"\n",
    "        img_flat = img.view(img.shape[0], -1)   # TODO: Figure out what this does \n",
    "        validity = self.model(img_flat)   # calculates validity score\n",
    "        #print(\"forward validity from discriminator: \" + str((np.max(np.abs(validity.detach().numpy())))))\n",
    "        return validity\n",
    "\n",
    "\n",
    "# ******* OUT OF CLASSES NOW ************\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    \n",
    "    \n",
    "# Configure data loader - CHANGE\n",
    "os.makedirs(\"./data/\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "   DiHiggsSignalMCDataset('./DiHiggs Data', generator_level = False),\n",
    "   batch_size=opt.batch_size,\n",
    "   shuffle=True,\n",
    ")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f3fa97836285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Class used for optimizers in the future. Defines all variables and stuff needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1737\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1738\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2392\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2393\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2380\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_ALP(D, real_samples, fake_samples):   # TODO: Find out why these are .data\n",
    "    \"\"\"\n",
    "    Calculates the gradient penalty loss for WGAN GP\n",
    "    D input will be discrimantor function\n",
    "    real_samples and fake_samples are from reality and generator. Both are sent in via memory location of buffer\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Random weight term for interpolation between real and fake samples (how much of each)\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0),1)))   # This is a tensor designating which to use where\n",
    "    #print(alpha)\n",
    "  #  print(alpha.shape)\n",
    "    # Get random interpolation between real and fake samples\n",
    "   # print(real_samples.shape)\n",
    "    \n",
    "    # Gets some of real and some of fake samples for gradient penalty calculation\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    # .requires grad is something attached to all tensors and can be used to speed up (by making false I think)\n",
    "    # It is automatically false, but if you need gradient then set to be true\n",
    "    # TODO: Understand how this statement works\n",
    "    \n",
    "    \n",
    "    ################## CALCULATE R ADVERSARIAL ###############################################\n",
    "    # start with random unit vector r0\n",
    "    r0 = np.random.rand(interpolates.shape[0], interpolates.shape[1])\n",
    "    r0 = Tensor(r0/r0.max(axis = 0)).requires_grad_(True)\n",
    "    #print(r[0])\n",
    "    \n",
    "    #  add this initial r to our random data points\n",
    "    interpol_y0 = (interpolates + opt.Xi * r0).requires_grad_(True)   #.requires_grad_(True)\n",
    "    # run the discriminator on both of these\n",
    "    d_interpolates = D(interpolates)   # Run discriminator on interpolates to get validity scores\n",
    "    d_interpol_y0 = D(interpol_y0)   # do the same for the adjusted interpolates to find r adversarial\n",
    "\n",
    "    \n",
    "    # find gradient(d(f(x) - f(x+r)))\n",
    "    difference = (d_interpolates - d_interpol_y0).requires_grad_(True)  #.requires_grad_(True)\n",
    "    #print(\"d interpolates: \" + str(d_interpolates.shape) + \" \" + str(d_interpolates.type))\n",
    "    #print(\"difference: \" + str(difference.shape) + \" \" + str(difference.type))\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False) \n",
    "    gradient_r0 = autograd.grad(\n",
    "        outputs=difference,\n",
    "        inputs=r0,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # finally, find r_adversarial!\n",
    "    epsilon_r = np.random.uniform(0.1,10)\n",
    "    r_adv = epsilon_r * gradient_r0/np.linalg.norm(gradient_r0.cpu().detach().numpy())\n",
    "    #print(np.max(np.linalg.norm(r_adv.cpu().detach().numpy())))\n",
    "###########################################################################################################\n",
    "\n",
    "######### Now find the loss ###########################\n",
    "    \n",
    "    interpol_adversarial = (interpolates + r_adv).requires_grad_(True)\n",
    "    d_interpol_adv = D(interpol_adversarial)\n",
    "    abs_difference = np.abs((d_interpolates - d_interpol_adv).cpu().detach().numpy())/ \\\n",
    "    (np.linalg.norm(r_adv.cpu().detach().numpy())) - 1\n",
    "    squared = np.square(np.maximum(abs_difference,np.zeros(100)))\n",
    "    #print(\"Max of alp before mean: \" + str(np.max(np.abs(squared))))\n",
    "    \n",
    "    alp_penalty = squared.mean()\n",
    "   # print(\"ALP final: \" + str(alp_penalty))\n",
    "    \n",
    "    return alp_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20000] [Batch 0/192] [D loss: 0.604934] [G loss: -6.642289]\n",
      "[Epoch 0/20000] [Batch 10/192] [D loss: 0.578657] [G loss: -8.013991]\n",
      "[Epoch 0/20000] [Batch 20/192] [D loss: 0.500111] [G loss: -7.837383]\n",
      "[Epoch 0/20000] [Batch 30/192] [D loss: 0.296897] [G loss: -6.350225]\n",
      "[Epoch 0/20000] [Batch 40/192] [D loss: -0.018437] [G loss: -4.221060]\n",
      "[Epoch 0/20000] [Batch 50/192] [D loss: -0.230345] [G loss: -3.197325]\n",
      "[Epoch 0/20000] [Batch 60/192] [D loss: -0.189514] [G loss: -3.848126]\n",
      "[Epoch 0/20000] [Batch 70/192] [D loss: -0.048276] [G loss: -4.697019]\n",
      "[Epoch 0/20000] [Batch 80/192] [D loss: 0.117612] [G loss: -3.215794]\n",
      "[Epoch 0/20000] [Batch 90/192] [D loss: 0.079117] [G loss: -1.266546]\n",
      "[Epoch 0/20000] [Batch 100/192] [D loss: -0.128019] [G loss: -0.325409]\n",
      "[Epoch 0/20000] [Batch 110/192] [D loss: -0.297198] [G loss: 0.785536]\n",
      "[Epoch 0/20000] [Batch 120/192] [D loss: -0.397040] [G loss: 2.727832]\n",
      "[Epoch 0/20000] [Batch 130/192] [D loss: -0.391507] [G loss: 3.362292]\n",
      "[Epoch 0/20000] [Batch 140/192] [D loss: -0.412223] [G loss: 4.447799]\n",
      "[Epoch 0/20000] [Batch 150/192] [D loss: -0.397903] [G loss: 5.787191]\n",
      "[Epoch 0/20000] [Batch 160/192] [D loss: -0.467290] [G loss: 6.077765]\n",
      "[Epoch 0/20000] [Batch 170/192] [D loss: -0.529187] [G loss: 6.779209]\n",
      "[Epoch 0/20000] [Batch 180/192] [D loss: -0.499689] [G loss: 7.490903]\n",
      "[Epoch 0/20000] [Batch 190/192] [D loss: -0.028394] [G loss: 6.831407]\n",
      "[Epoch 1/20000] [Batch 5/192] [D loss: 1.084512] [G loss: 5.096450]\n",
      "[Epoch 1/20000] [Batch 15/192] [D loss: 1.561967] [G loss: 2.567227]\n",
      "[Epoch 1/20000] [Batch 25/192] [D loss: 1.213761] [G loss: 1.445925]\n",
      "[Epoch 1/20000] [Batch 35/192] [D loss: 0.770674] [G loss: 0.385919]\n",
      "[Epoch 1/20000] [Batch 45/192] [D loss: 0.224126] [G loss: -0.885994]\n",
      "[Epoch 1/20000] [Batch 55/192] [D loss: -0.345444] [G loss: -1.889745]\n",
      "[Epoch 1/20000] [Batch 65/192] [D loss: -0.798848] [G loss: -2.655408]\n",
      "[Epoch 1/20000] [Batch 75/192] [D loss: -1.041370] [G loss: -3.086491]\n",
      "[Epoch 1/20000] [Batch 85/192] [D loss: -0.729317] [G loss: -3.760269]\n",
      "[Epoch 1/20000] [Batch 95/192] [D loss: -0.169595] [G loss: -3.547551]\n",
      "[Epoch 1/20000] [Batch 105/192] [D loss: 0.012855] [G loss: -2.893917]\n",
      "[Epoch 1/20000] [Batch 115/192] [D loss: 0.382966] [G loss: -2.514532]\n",
      "[Epoch 1/20000] [Batch 125/192] [D loss: 0.457936] [G loss: -2.678056]\n",
      "[Epoch 1/20000] [Batch 135/192] [D loss: 0.202809] [G loss: -1.698906]\n",
      "[Epoch 1/20000] [Batch 145/192] [D loss: 0.188715] [G loss: -1.851749]\n",
      "[Epoch 1/20000] [Batch 155/192] [D loss: 0.198766] [G loss: 0.209104]\n",
      "[Epoch 1/20000] [Batch 165/192] [D loss: 0.136219] [G loss: 1.351664]\n",
      "[Epoch 1/20000] [Batch 175/192] [D loss: -0.007270] [G loss: 3.160105]\n",
      "[Epoch 1/20000] [Batch 185/192] [D loss: -0.161678] [G loss: 5.214681]\n",
      "[Epoch 2/20000] [Batch 0/192] [D loss: -0.153840] [G loss: 5.648405]\n",
      "[Epoch 2/20000] [Batch 10/192] [D loss: -0.183890] [G loss: 6.370155]\n",
      "[Epoch 2/20000] [Batch 20/192] [D loss: -0.151964] [G loss: 6.740987]\n",
      "[Epoch 2/20000] [Batch 30/192] [D loss: -0.078090] [G loss: 7.016454]\n",
      "[Epoch 2/20000] [Batch 40/192] [D loss: 0.241593] [G loss: 5.793793]\n",
      "[Epoch 2/20000] [Batch 50/192] [D loss: 0.184572] [G loss: 4.111691]\n",
      "[Epoch 2/20000] [Batch 60/192] [D loss: -0.097862] [G loss: 3.710793]\n",
      "[Epoch 2/20000] [Batch 70/192] [D loss: -0.255908] [G loss: 3.045321]\n",
      "[Epoch 2/20000] [Batch 80/192] [D loss: -0.244718] [G loss: 2.949205]\n",
      "[Epoch 2/20000] [Batch 90/192] [D loss: -0.219364] [G loss: 4.023633]\n",
      "[Epoch 2/20000] [Batch 100/192] [D loss: -0.062253] [G loss: 4.897894]\n",
      "[Epoch 2/20000] [Batch 110/192] [D loss: 0.029300] [G loss: 4.406924]\n",
      "[Epoch 2/20000] [Batch 120/192] [D loss: -0.097156] [G loss: 4.429596]\n",
      "[Epoch 2/20000] [Batch 130/192] [D loss: -0.167517] [G loss: 4.402466]\n",
      "[Epoch 2/20000] [Batch 140/192] [D loss: -0.101102] [G loss: 6.768114]\n",
      "[Epoch 2/20000] [Batch 150/192] [D loss: -0.146238] [G loss: 7.162267]\n",
      "[Epoch 2/20000] [Batch 160/192] [D loss: -0.251443] [G loss: 8.695602]\n",
      "[Epoch 2/20000] [Batch 170/192] [D loss: -0.240774] [G loss: 10.491770]\n",
      "[Epoch 2/20000] [Batch 180/192] [D loss: -0.072430] [G loss: 9.478859]\n",
      "[Epoch 2/20000] [Batch 190/192] [D loss: -0.099525] [G loss: 8.300998]\n",
      "[Epoch 3/20000] [Batch 5/192] [D loss: -0.214809] [G loss: 8.388115]\n",
      "[Epoch 3/20000] [Batch 15/192] [D loss: -0.170086] [G loss: 7.437832]\n",
      "[Epoch 3/20000] [Batch 25/192] [D loss: -0.066417] [G loss: 7.688959]\n",
      "[Epoch 3/20000] [Batch 35/192] [D loss: -0.142666] [G loss: 8.856782]\n",
      "[Epoch 3/20000] [Batch 45/192] [D loss: -0.180430] [G loss: 10.782640]\n",
      "[Epoch 3/20000] [Batch 55/192] [D loss: -0.229104] [G loss: 11.795522]\n",
      "[Epoch 3/20000] [Batch 65/192] [D loss: -0.300370] [G loss: 12.825006]\n",
      "[Epoch 3/20000] [Batch 75/192] [D loss: -0.214357] [G loss: 13.524374]\n",
      "[Epoch 3/20000] [Batch 85/192] [D loss: -0.165088] [G loss: 12.923567]\n",
      "[Epoch 3/20000] [Batch 95/192] [D loss: -0.206238] [G loss: 11.741725]\n",
      "[Epoch 3/20000] [Batch 105/192] [D loss: -0.255915] [G loss: 10.655108]\n",
      "[Epoch 3/20000] [Batch 115/192] [D loss: -0.170034] [G loss: 12.858427]\n",
      "[Epoch 3/20000] [Batch 125/192] [D loss: -0.174681] [G loss: 13.191018]\n",
      "[Epoch 3/20000] [Batch 135/192] [D loss: -0.129485] [G loss: 12.981817]\n",
      "[Epoch 3/20000] [Batch 145/192] [D loss: 0.041111] [G loss: 10.869629]\n",
      "[Epoch 3/20000] [Batch 155/192] [D loss: 0.171295] [G loss: 9.491121]\n",
      "[Epoch 3/20000] [Batch 165/192] [D loss: -0.230811] [G loss: 7.799263]\n",
      "[Epoch 3/20000] [Batch 175/192] [D loss: -0.480981] [G loss: 6.349767]\n",
      "[Epoch 3/20000] [Batch 185/192] [D loss: -0.402515] [G loss: 5.529528]\n",
      "[Epoch 4/20000] [Batch 0/192] [D loss: -0.149263] [G loss: 5.599408]\n",
      "[Epoch 4/20000] [Batch 10/192] [D loss: -0.124677] [G loss: 5.257486]\n",
      "[Epoch 4/20000] [Batch 20/192] [D loss: -0.105170] [G loss: 5.439269]\n",
      "[Epoch 4/20000] [Batch 30/192] [D loss: -0.053961] [G loss: 6.940496]\n",
      "[Epoch 4/20000] [Batch 40/192] [D loss: -0.006433] [G loss: 6.719777]\n",
      "[Epoch 4/20000] [Batch 50/192] [D loss: 0.094154] [G loss: 7.616339]\n",
      "[Epoch 4/20000] [Batch 60/192] [D loss: -0.023116] [G loss: 6.172277]\n",
      "[Epoch 4/20000] [Batch 70/192] [D loss: -0.226856] [G loss: 5.054618]\n",
      "[Epoch 4/20000] [Batch 80/192] [D loss: -0.254898] [G loss: 6.470266]\n",
      "[Epoch 4/20000] [Batch 90/192] [D loss: -0.271608] [G loss: 6.240679]\n",
      "[Epoch 4/20000] [Batch 100/192] [D loss: -0.120885] [G loss: 6.857604]\n",
      "[Epoch 4/20000] [Batch 110/192] [D loss: -0.176506] [G loss: 7.943468]\n",
      "[Epoch 4/20000] [Batch 120/192] [D loss: -0.121363] [G loss: 8.790009]\n",
      "[Epoch 4/20000] [Batch 130/192] [D loss: -0.212945] [G loss: 8.143423]\n",
      "[Epoch 4/20000] [Batch 140/192] [D loss: -0.162935] [G loss: 12.339057]\n",
      "[Epoch 4/20000] [Batch 150/192] [D loss: -0.100543] [G loss: 10.146674]\n",
      "[Epoch 4/20000] [Batch 160/192] [D loss: -0.073539] [G loss: 10.161999]\n",
      "[Epoch 4/20000] [Batch 170/192] [D loss: -0.158049] [G loss: 10.084675]\n",
      "[Epoch 4/20000] [Batch 180/192] [D loss: -0.214825] [G loss: 8.727047]\n",
      "[Epoch 4/20000] [Batch 190/192] [D loss: -0.152392] [G loss: 9.455391]\n",
      "[Epoch 5/20000] [Batch 5/192] [D loss: -0.151344] [G loss: 7.809890]\n",
      "[Epoch 5/20000] [Batch 15/192] [D loss: -0.135695] [G loss: 10.569151]\n",
      "[Epoch 5/20000] [Batch 25/192] [D loss: -0.163771] [G loss: 10.796336]\n",
      "[Epoch 5/20000] [Batch 35/192] [D loss: -0.154716] [G loss: 10.182941]\n",
      "[Epoch 5/20000] [Batch 45/192] [D loss: -0.092000] [G loss: 9.644622]\n",
      "[Epoch 5/20000] [Batch 55/192] [D loss: -0.146894] [G loss: 9.384122]\n",
      "[Epoch 5/20000] [Batch 65/192] [D loss: -0.145288] [G loss: 7.828283]\n",
      "[Epoch 5/20000] [Batch 75/192] [D loss: -0.169943] [G loss: 6.660539]\n",
      "[Epoch 5/20000] [Batch 85/192] [D loss: -0.152745] [G loss: 8.465816]\n",
      "[Epoch 5/20000] [Batch 95/192] [D loss: -0.187149] [G loss: 7.783326]\n",
      "[Epoch 5/20000] [Batch 105/192] [D loss: -0.131908] [G loss: 8.030894]\n",
      "[Epoch 5/20000] [Batch 115/192] [D loss: -0.156760] [G loss: 10.186299]\n",
      "[Epoch 5/20000] [Batch 125/192] [D loss: -0.200081] [G loss: 9.452843]\n",
      "[Epoch 5/20000] [Batch 135/192] [D loss: -0.191256] [G loss: 8.221522]\n",
      "[Epoch 5/20000] [Batch 145/192] [D loss: -0.181281] [G loss: 10.721690]\n",
      "[Epoch 5/20000] [Batch 155/192] [D loss: -0.155090] [G loss: 11.574822]\n",
      "[Epoch 5/20000] [Batch 165/192] [D loss: -0.112622] [G loss: 9.598412]\n",
      "[Epoch 5/20000] [Batch 175/192] [D loss: -0.048616] [G loss: 10.067901]\n",
      "[Epoch 5/20000] [Batch 185/192] [D loss: -0.065795] [G loss: 10.245100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/20000] [Batch 0/192] [D loss: -0.005962] [G loss: 10.589444]\n",
      "[Epoch 6/20000] [Batch 10/192] [D loss: -0.104293] [G loss: 11.610182]\n",
      "[Epoch 6/20000] [Batch 20/192] [D loss: -0.140729] [G loss: 8.688994]\n",
      "[Epoch 6/20000] [Batch 30/192] [D loss: -0.157740] [G loss: 10.643454]\n",
      "[Epoch 6/20000] [Batch 40/192] [D loss: -0.174800] [G loss: 9.535302]\n",
      "[Epoch 6/20000] [Batch 50/192] [D loss: -0.170429] [G loss: 10.055379]\n",
      "[Epoch 6/20000] [Batch 60/192] [D loss: -0.200838] [G loss: 11.210100]\n",
      "[Epoch 6/20000] [Batch 70/192] [D loss: -0.194701] [G loss: 9.948377]\n",
      "[Epoch 6/20000] [Batch 80/192] [D loss: -0.199996] [G loss: 11.272081]\n",
      "[Epoch 6/20000] [Batch 90/192] [D loss: -0.240033] [G loss: 11.380981]\n",
      "[Epoch 6/20000] [Batch 100/192] [D loss: -0.274364] [G loss: 10.467807]\n",
      "[Epoch 6/20000] [Batch 110/192] [D loss: -0.272298] [G loss: 11.119071]\n",
      "[Epoch 6/20000] [Batch 120/192] [D loss: -0.238013] [G loss: 12.129158]\n",
      "[Epoch 6/20000] [Batch 130/192] [D loss: -0.206676] [G loss: 11.555389]\n",
      "[Epoch 6/20000] [Batch 140/192] [D loss: -0.245078] [G loss: 11.496582]\n",
      "[Epoch 6/20000] [Batch 150/192] [D loss: -0.166554] [G loss: 12.746731]\n",
      "[Epoch 6/20000] [Batch 160/192] [D loss: -0.190238] [G loss: 9.920417]\n",
      "[Epoch 6/20000] [Batch 170/192] [D loss: -0.276636] [G loss: 11.768615]\n",
      "[Epoch 6/20000] [Batch 180/192] [D loss: -0.253459] [G loss: 11.229471]\n",
      "[Epoch 6/20000] [Batch 190/192] [D loss: -0.311067] [G loss: 11.847757]\n",
      "[Epoch 7/20000] [Batch 5/192] [D loss: -0.214999] [G loss: 10.522323]\n",
      "[Epoch 7/20000] [Batch 15/192] [D loss: -0.185729] [G loss: 9.100533]\n",
      "[Epoch 7/20000] [Batch 25/192] [D loss: -0.173791] [G loss: 11.279207]\n",
      "[Epoch 7/20000] [Batch 35/192] [D loss: -0.109984] [G loss: 12.556034]\n",
      "[Epoch 7/20000] [Batch 45/192] [D loss: -0.128037] [G loss: 10.983506]\n",
      "[Epoch 7/20000] [Batch 55/192] [D loss: -0.111820] [G loss: 11.944118]\n",
      "[Epoch 7/20000] [Batch 65/192] [D loss: -0.135390] [G loss: 12.135212]\n",
      "[Epoch 7/20000] [Batch 75/192] [D loss: -0.166965] [G loss: 10.966262]\n",
      "[Epoch 7/20000] [Batch 85/192] [D loss: -0.202765] [G loss: 12.300943]\n",
      "[Epoch 7/20000] [Batch 95/192] [D loss: -0.144459] [G loss: 12.320664]\n",
      "[Epoch 7/20000] [Batch 105/192] [D loss: -0.193452] [G loss: 9.995679]\n",
      "[Epoch 7/20000] [Batch 115/192] [D loss: -0.210729] [G loss: 12.732645]\n",
      "[Epoch 7/20000] [Batch 125/192] [D loss: -0.212831] [G loss: 12.763447]\n",
      "[Epoch 7/20000] [Batch 135/192] [D loss: -0.213715] [G loss: 11.914362]\n",
      "[Epoch 7/20000] [Batch 145/192] [D loss: -0.189806] [G loss: 11.402793]\n",
      "[Epoch 7/20000] [Batch 155/192] [D loss: -0.149772] [G loss: 12.187464]\n",
      "[Epoch 7/20000] [Batch 165/192] [D loss: -0.207335] [G loss: 13.476094]\n",
      "[Epoch 7/20000] [Batch 175/192] [D loss: -0.176484] [G loss: 12.947429]\n",
      "[Epoch 7/20000] [Batch 185/192] [D loss: -0.242270] [G loss: 12.978549]\n",
      "[Epoch 8/20000] [Batch 0/192] [D loss: -0.080658] [G loss: 12.188703]\n",
      "[Epoch 8/20000] [Batch 10/192] [D loss: -0.162059] [G loss: 11.455015]\n",
      "[Epoch 8/20000] [Batch 20/192] [D loss: -0.174702] [G loss: 13.403030]\n",
      "[Epoch 8/20000] [Batch 30/192] [D loss: -0.183373] [G loss: 10.287206]\n",
      "[Epoch 8/20000] [Batch 40/192] [D loss: -0.278296] [G loss: 11.290627]\n",
      "[Epoch 8/20000] [Batch 50/192] [D loss: -0.271223] [G loss: 13.176897]\n",
      "[Epoch 8/20000] [Batch 60/192] [D loss: -0.213022] [G loss: 10.550929]\n",
      "[Epoch 8/20000] [Batch 70/192] [D loss: -0.186644] [G loss: 11.816439]\n",
      "[Epoch 8/20000] [Batch 80/192] [D loss: -0.204367] [G loss: 13.614314]\n",
      "[Epoch 8/20000] [Batch 90/192] [D loss: -0.187042] [G loss: 12.189713]\n",
      "[Epoch 8/20000] [Batch 100/192] [D loss: -0.183927] [G loss: 13.584810]\n",
      "[Epoch 8/20000] [Batch 110/192] [D loss: -0.119368] [G loss: 13.897515]\n",
      "[Epoch 8/20000] [Batch 120/192] [D loss: -0.110445] [G loss: 12.249166]\n",
      "[Epoch 8/20000] [Batch 130/192] [D loss: -0.140037] [G loss: 13.114671]\n",
      "[Epoch 8/20000] [Batch 140/192] [D loss: -0.144821] [G loss: 14.222768]\n",
      "[Epoch 8/20000] [Batch 150/192] [D loss: -0.149501] [G loss: 12.585774]\n",
      "[Epoch 8/20000] [Batch 160/192] [D loss: -0.153013] [G loss: 12.587420]\n",
      "[Epoch 8/20000] [Batch 170/192] [D loss: -0.192157] [G loss: 14.377151]\n",
      "[Epoch 8/20000] [Batch 180/192] [D loss: -0.188564] [G loss: 14.213626]\n",
      "[Epoch 8/20000] [Batch 190/192] [D loss: -0.137059] [G loss: 12.400640]\n",
      "[Epoch 9/20000] [Batch 5/192] [D loss: -0.267369] [G loss: 16.527597]\n",
      "[Epoch 9/20000] [Batch 15/192] [D loss: -0.171670] [G loss: 12.259542]\n",
      "[Epoch 9/20000] [Batch 25/192] [D loss: -0.190753] [G loss: 14.877165]\n",
      "[Epoch 9/20000] [Batch 35/192] [D loss: -0.120577] [G loss: 12.262807]\n",
      "[Epoch 9/20000] [Batch 45/192] [D loss: -0.226088] [G loss: 14.482405]\n",
      "[Epoch 9/20000] [Batch 55/192] [D loss: -0.112527] [G loss: 13.256880]\n",
      "[Epoch 9/20000] [Batch 65/192] [D loss: -0.158544] [G loss: 12.679430]\n",
      "[Epoch 9/20000] [Batch 75/192] [D loss: -0.241858] [G loss: 14.387401]\n",
      "[Epoch 9/20000] [Batch 85/192] [D loss: -0.239578] [G loss: 10.814708]\n",
      "[Epoch 9/20000] [Batch 95/192] [D loss: -0.189023] [G loss: 16.180025]\n",
      "[Epoch 9/20000] [Batch 105/192] [D loss: -0.155384] [G loss: 12.682172]\n",
      "[Epoch 9/20000] [Batch 115/192] [D loss: -0.150088] [G loss: 15.986512]\n",
      "[Epoch 9/20000] [Batch 125/192] [D loss: -0.151733] [G loss: 13.815094]\n",
      "[Epoch 9/20000] [Batch 135/192] [D loss: -0.207252] [G loss: 14.559489]\n",
      "[Epoch 9/20000] [Batch 145/192] [D loss: -0.172200] [G loss: 12.859915]\n",
      "[Epoch 9/20000] [Batch 155/192] [D loss: -0.140721] [G loss: 15.333155]\n",
      "[Epoch 9/20000] [Batch 165/192] [D loss: -0.213037] [G loss: 14.943366]\n",
      "[Epoch 9/20000] [Batch 175/192] [D loss: -0.258820] [G loss: 14.270032]\n",
      "[Epoch 9/20000] [Batch 185/192] [D loss: -0.294904] [G loss: 15.406811]\n",
      "[Epoch 10/20000] [Batch 0/192] [D loss: -0.116545] [G loss: 14.489820]\n",
      "[Epoch 10/20000] [Batch 10/192] [D loss: -0.168073] [G loss: 14.147977]\n",
      "[Epoch 10/20000] [Batch 20/192] [D loss: -0.119922] [G loss: 15.805417]\n",
      "[Epoch 10/20000] [Batch 30/192] [D loss: -0.036966] [G loss: 16.129957]\n",
      "[Epoch 10/20000] [Batch 40/192] [D loss: -0.076220] [G loss: 15.465184]\n",
      "[Epoch 10/20000] [Batch 50/192] [D loss: -0.107594] [G loss: 14.372985]\n",
      "[Epoch 10/20000] [Batch 60/192] [D loss: -0.232010] [G loss: 16.354536]\n",
      "[Epoch 10/20000] [Batch 70/192] [D loss: -0.234916] [G loss: 12.965586]\n",
      "[Epoch 10/20000] [Batch 80/192] [D loss: -0.271297] [G loss: 12.542262]\n",
      "[Epoch 10/20000] [Batch 90/192] [D loss: -0.261293] [G loss: 13.713969]\n",
      "[Epoch 10/20000] [Batch 100/192] [D loss: -0.234221] [G loss: 14.662122]\n",
      "[Epoch 10/20000] [Batch 110/192] [D loss: -0.118680] [G loss: 13.954043]\n",
      "[Epoch 10/20000] [Batch 120/192] [D loss: -0.172277] [G loss: 13.422300]\n",
      "[Epoch 10/20000] [Batch 130/192] [D loss: -0.214658] [G loss: 14.727758]\n",
      "[Epoch 10/20000] [Batch 140/192] [D loss: -0.270270] [G loss: 13.963621]\n",
      "[Epoch 10/20000] [Batch 150/192] [D loss: -0.203124] [G loss: 15.311997]\n",
      "[Epoch 10/20000] [Batch 160/192] [D loss: -0.248178] [G loss: 16.820917]\n",
      "[Epoch 10/20000] [Batch 170/192] [D loss: -0.139697] [G loss: 14.362320]\n",
      "[Epoch 10/20000] [Batch 180/192] [D loss: -0.199386] [G loss: 13.255106]\n",
      "[Epoch 10/20000] [Batch 190/192] [D loss: -0.311590] [G loss: 16.308189]\n",
      "[Epoch 11/20000] [Batch 5/192] [D loss: -0.256077] [G loss: 13.383917]\n",
      "[Epoch 11/20000] [Batch 15/192] [D loss: -0.169050] [G loss: 17.123110]\n",
      "[Epoch 11/20000] [Batch 25/192] [D loss: -0.174926] [G loss: 15.374826]\n",
      "[Epoch 11/20000] [Batch 35/192] [D loss: -0.200322] [G loss: 13.539946]\n",
      "[Epoch 11/20000] [Batch 45/192] [D loss: -0.268536] [G loss: 16.992872]\n",
      "[Epoch 11/20000] [Batch 55/192] [D loss: -0.167093] [G loss: 16.298405]\n",
      "[Epoch 11/20000] [Batch 65/192] [D loss: -0.250927] [G loss: 16.345358]\n",
      "[Epoch 11/20000] [Batch 75/192] [D loss: -0.266214] [G loss: 16.944258]\n",
      "[Epoch 11/20000] [Batch 85/192] [D loss: -0.241636] [G loss: 14.990523]\n",
      "[Epoch 11/20000] [Batch 95/192] [D loss: -0.250775] [G loss: 15.477483]\n",
      "[Epoch 11/20000] [Batch 105/192] [D loss: -0.088079] [G loss: 15.761558]\n",
      "[Epoch 11/20000] [Batch 115/192] [D loss: -0.109144] [G loss: 17.831379]\n",
      "[Epoch 11/20000] [Batch 125/192] [D loss: -0.148180] [G loss: 17.216942]\n",
      "[Epoch 11/20000] [Batch 135/192] [D loss: -0.204163] [G loss: 15.163319]\n",
      "[Epoch 11/20000] [Batch 145/192] [D loss: -0.211033] [G loss: 16.833723]\n",
      "[Epoch 11/20000] [Batch 155/192] [D loss: -0.242025] [G loss: 17.344206]\n",
      "[Epoch 11/20000] [Batch 165/192] [D loss: -0.252890] [G loss: 16.400339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/20000] [Batch 175/192] [D loss: -0.190287] [G loss: 17.639114]\n",
      "[Epoch 11/20000] [Batch 185/192] [D loss: -0.273409] [G loss: 18.597532]\n",
      "[Epoch 12/20000] [Batch 0/192] [D loss: -0.041637] [G loss: 16.088005]\n",
      "[Epoch 12/20000] [Batch 10/192] [D loss: -0.228434] [G loss: 17.940094]\n",
      "[Epoch 12/20000] [Batch 20/192] [D loss: -0.220020] [G loss: 19.304050]\n",
      "[Epoch 12/20000] [Batch 30/192] [D loss: -0.248777] [G loss: 17.114403]\n",
      "[Epoch 12/20000] [Batch 40/192] [D loss: -0.198954] [G loss: 16.356569]\n",
      "[Epoch 12/20000] [Batch 50/192] [D loss: -0.255960] [G loss: 19.133614]\n",
      "[Epoch 12/20000] [Batch 60/192] [D loss: -0.173138] [G loss: 17.787251]\n",
      "[Epoch 12/20000] [Batch 70/192] [D loss: -0.227362] [G loss: 18.206762]\n",
      "[Epoch 12/20000] [Batch 80/192] [D loss: -0.313465] [G loss: 20.368313]\n",
      "[Epoch 12/20000] [Batch 90/192] [D loss: -0.304316] [G loss: 18.738594]\n",
      "[Epoch 12/20000] [Batch 100/192] [D loss: -0.431522] [G loss: 14.824829]\n",
      "[Epoch 12/20000] [Batch 110/192] [D loss: -0.233015] [G loss: 19.382900]\n",
      "[Epoch 12/20000] [Batch 120/192] [D loss: -0.325401] [G loss: 19.167471]\n",
      "[Epoch 12/20000] [Batch 130/192] [D loss: -0.232174] [G loss: 16.053436]\n",
      "[Epoch 12/20000] [Batch 140/192] [D loss: -0.406179] [G loss: 15.779358]\n",
      "[Epoch 12/20000] [Batch 150/192] [D loss: -0.283514] [G loss: 19.606209]\n",
      "[Epoch 12/20000] [Batch 160/192] [D loss: -0.427616] [G loss: 16.031986]\n",
      "[Epoch 12/20000] [Batch 170/192] [D loss: -0.285702] [G loss: 17.358719]\n",
      "[Epoch 12/20000] [Batch 180/192] [D loss: -0.315626] [G loss: 16.775021]\n",
      "[Epoch 12/20000] [Batch 190/192] [D loss: -0.223421] [G loss: 19.075483]\n",
      "[Epoch 13/20000] [Batch 5/192] [D loss: -0.289803] [G loss: 16.049469]\n",
      "[Epoch 13/20000] [Batch 15/192] [D loss: -0.117582] [G loss: 19.289518]\n",
      "[Epoch 13/20000] [Batch 25/192] [D loss: -0.322763] [G loss: 19.976719]\n",
      "[Epoch 13/20000] [Batch 35/192] [D loss: -0.173691] [G loss: 17.121286]\n",
      "[Epoch 13/20000] [Batch 45/192] [D loss: -0.274454] [G loss: 19.977194]\n",
      "[Epoch 13/20000] [Batch 55/192] [D loss: -0.214342] [G loss: 17.584141]\n",
      "[Epoch 13/20000] [Batch 65/192] [D loss: -0.290066] [G loss: 17.807468]\n",
      "[Epoch 13/20000] [Batch 75/192] [D loss: -0.210825] [G loss: 17.077597]\n",
      "[Epoch 13/20000] [Batch 85/192] [D loss: -0.294397] [G loss: 18.663595]\n",
      "[Epoch 13/20000] [Batch 95/192] [D loss: -0.277658] [G loss: 21.900627]\n",
      "[Epoch 13/20000] [Batch 105/192] [D loss: -0.213673] [G loss: 16.330725]\n",
      "[Epoch 13/20000] [Batch 115/192] [D loss: -0.224016] [G loss: 20.014565]\n",
      "[Epoch 13/20000] [Batch 125/192] [D loss: -0.209949] [G loss: 21.701204]\n",
      "[Epoch 13/20000] [Batch 135/192] [D loss: -0.220682] [G loss: 20.253546]\n",
      "[Epoch 13/20000] [Batch 145/192] [D loss: -0.322203] [G loss: 20.044899]\n",
      "[Epoch 13/20000] [Batch 155/192] [D loss: -0.243307] [G loss: 18.411608]\n",
      "[Epoch 13/20000] [Batch 165/192] [D loss: -0.217905] [G loss: 18.734051]\n",
      "[Epoch 13/20000] [Batch 175/192] [D loss: -0.152975] [G loss: 18.586048]\n",
      "[Epoch 13/20000] [Batch 185/192] [D loss: -0.165997] [G loss: 17.422138]\n",
      "[Epoch 14/20000] [Batch 0/192] [D loss: 0.018068] [G loss: 18.552290]\n",
      "[Epoch 14/20000] [Batch 10/192] [D loss: -0.183187] [G loss: 20.889147]\n",
      "[Epoch 14/20000] [Batch 20/192] [D loss: -0.254940] [G loss: 18.586451]\n",
      "[Epoch 14/20000] [Batch 30/192] [D loss: -0.279243] [G loss: 20.354872]\n",
      "[Epoch 14/20000] [Batch 40/192] [D loss: -0.219780] [G loss: 21.539083]\n",
      "[Epoch 14/20000] [Batch 50/192] [D loss: -0.242647] [G loss: 19.238173]\n",
      "[Epoch 14/20000] [Batch 60/192] [D loss: -0.215816] [G loss: 19.510206]\n",
      "[Epoch 14/20000] [Batch 70/192] [D loss: -0.281610] [G loss: 18.987047]\n",
      "[Epoch 14/20000] [Batch 80/192] [D loss: -0.214645] [G loss: 19.196180]\n",
      "[Epoch 14/20000] [Batch 90/192] [D loss: -0.221130] [G loss: 19.966642]\n",
      "[Epoch 14/20000] [Batch 100/192] [D loss: -0.297266] [G loss: 17.114119]\n",
      "[Epoch 14/20000] [Batch 110/192] [D loss: -0.230442] [G loss: 20.646875]\n",
      "[Epoch 14/20000] [Batch 120/192] [D loss: -0.143459] [G loss: 21.064312]\n",
      "[Epoch 14/20000] [Batch 130/192] [D loss: -0.127729] [G loss: 21.779455]\n",
      "[Epoch 14/20000] [Batch 140/192] [D loss: -0.081297] [G loss: 18.619677]\n",
      "[Epoch 14/20000] [Batch 150/192] [D loss: -0.169146] [G loss: 18.670435]\n",
      "[Epoch 14/20000] [Batch 160/192] [D loss: -0.113548] [G loss: 22.140732]\n",
      "[Epoch 14/20000] [Batch 170/192] [D loss: -0.189341] [G loss: 20.776541]\n",
      "[Epoch 14/20000] [Batch 180/192] [D loss: -0.162865] [G loss: 19.889547]\n",
      "[Epoch 14/20000] [Batch 190/192] [D loss: -0.275660] [G loss: 21.624851]\n",
      "[Epoch 15/20000] [Batch 5/192] [D loss: -0.254532] [G loss: 18.963634]\n",
      "[Epoch 15/20000] [Batch 15/192] [D loss: -0.200829] [G loss: 21.064247]\n",
      "[Epoch 15/20000] [Batch 25/192] [D loss: -0.178827] [G loss: 22.292864]\n",
      "[Epoch 15/20000] [Batch 35/192] [D loss: -0.191313] [G loss: 20.639805]\n",
      "[Epoch 15/20000] [Batch 45/192] [D loss: -0.259130] [G loss: 18.135994]\n",
      "[Epoch 15/20000] [Batch 55/192] [D loss: -0.274654] [G loss: 24.580433]\n",
      "[Epoch 15/20000] [Batch 65/192] [D loss: -0.270597] [G loss: 20.223314]\n",
      "[Epoch 15/20000] [Batch 75/192] [D loss: -0.129412] [G loss: 22.439373]\n",
      "[Epoch 15/20000] [Batch 85/192] [D loss: -0.202536] [G loss: 20.575974]\n",
      "[Epoch 15/20000] [Batch 95/192] [D loss: -0.182936] [G loss: 22.362566]\n",
      "[Epoch 15/20000] [Batch 105/192] [D loss: -0.265234] [G loss: 19.064255]\n",
      "[Epoch 15/20000] [Batch 115/192] [D loss: -0.079782] [G loss: 20.941595]\n",
      "[Epoch 15/20000] [Batch 125/192] [D loss: -0.233633] [G loss: 23.111139]\n",
      "[Epoch 15/20000] [Batch 135/192] [D loss: -0.114031] [G loss: 21.986969]\n",
      "[Epoch 15/20000] [Batch 145/192] [D loss: -0.132931] [G loss: 20.596888]\n",
      "[Epoch 15/20000] [Batch 155/192] [D loss: -0.269035] [G loss: 20.800564]\n",
      "[Epoch 15/20000] [Batch 165/192] [D loss: -0.235579] [G loss: 20.734171]\n",
      "[Epoch 15/20000] [Batch 175/192] [D loss: -0.221445] [G loss: 23.075607]\n",
      "[Epoch 15/20000] [Batch 185/192] [D loss: -0.242670] [G loss: 20.733847]\n",
      "[Epoch 16/20000] [Batch 0/192] [D loss: 0.099342] [G loss: 20.427177]\n",
      "[Epoch 16/20000] [Batch 10/192] [D loss: -0.231630] [G loss: 24.952076]\n",
      "[Epoch 16/20000] [Batch 20/192] [D loss: -0.128756] [G loss: 21.963142]\n",
      "[Epoch 16/20000] [Batch 30/192] [D loss: -0.193693] [G loss: 20.398989]\n",
      "[Epoch 16/20000] [Batch 40/192] [D loss: -0.217491] [G loss: 18.990364]\n",
      "[Epoch 16/20000] [Batch 50/192] [D loss: -0.283531] [G loss: 22.017727]\n",
      "[Epoch 16/20000] [Batch 60/192] [D loss: -0.188023] [G loss: 23.028294]\n",
      "[Epoch 16/20000] [Batch 70/192] [D loss: -0.221762] [G loss: 25.292564]\n",
      "[Epoch 16/20000] [Batch 80/192] [D loss: -0.290096] [G loss: 22.354115]\n",
      "[Epoch 16/20000] [Batch 90/192] [D loss: -0.310946] [G loss: 23.441282]\n",
      "[Epoch 16/20000] [Batch 100/192] [D loss: -0.002348] [G loss: 23.333899]\n",
      "[Epoch 16/20000] [Batch 110/192] [D loss: -0.240334] [G loss: 20.219688]\n",
      "[Epoch 16/20000] [Batch 120/192] [D loss: -0.068319] [G loss: 23.150497]\n",
      "[Epoch 16/20000] [Batch 130/192] [D loss: -0.253960] [G loss: 23.972279]\n",
      "[Epoch 16/20000] [Batch 140/192] [D loss: -0.236197] [G loss: 20.986372]\n",
      "[Epoch 16/20000] [Batch 150/192] [D loss: -0.253609] [G loss: 22.390059]\n",
      "[Epoch 16/20000] [Batch 160/192] [D loss: -0.307060] [G loss: 25.954056]\n",
      "[Epoch 16/20000] [Batch 170/192] [D loss: -0.290258] [G loss: 24.078049]\n",
      "[Epoch 16/20000] [Batch 180/192] [D loss: -0.131878] [G loss: 22.874226]\n",
      "[Epoch 16/20000] [Batch 190/192] [D loss: -0.299599] [G loss: 21.533676]\n",
      "[Epoch 17/20000] [Batch 5/192] [D loss: -0.232193] [G loss: 17.823877]\n",
      "[Epoch 17/20000] [Batch 15/192] [D loss: -0.249241] [G loss: 22.445494]\n",
      "[Epoch 17/20000] [Batch 25/192] [D loss: -0.262756] [G loss: 20.733376]\n",
      "[Epoch 17/20000] [Batch 35/192] [D loss: -0.277462] [G loss: 23.742012]\n",
      "[Epoch 17/20000] [Batch 45/192] [D loss: -0.274101] [G loss: 22.917667]\n",
      "[Epoch 17/20000] [Batch 55/192] [D loss: -0.269772] [G loss: 23.962831]\n",
      "[Epoch 17/20000] [Batch 65/192] [D loss: -0.309494] [G loss: 24.168741]\n",
      "[Epoch 17/20000] [Batch 75/192] [D loss: -0.305813] [G loss: 23.292133]\n",
      "[Epoch 17/20000] [Batch 85/192] [D loss: -0.286686] [G loss: 26.295815]\n",
      "[Epoch 17/20000] [Batch 95/192] [D loss: -0.206457] [G loss: 23.638737]\n",
      "[Epoch 17/20000] [Batch 105/192] [D loss: -0.246836] [G loss: 19.889153]\n",
      "[Epoch 17/20000] [Batch 115/192] [D loss: -0.249865] [G loss: 22.659376]\n",
      "[Epoch 17/20000] [Batch 125/192] [D loss: -0.323677] [G loss: 27.414810]\n",
      "[Epoch 17/20000] [Batch 135/192] [D loss: -0.311813] [G loss: 25.229832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/20000] [Batch 145/192] [D loss: -0.319660] [G loss: 24.496155]\n",
      "[Epoch 17/20000] [Batch 155/192] [D loss: -0.264248] [G loss: 24.460323]\n",
      "[Epoch 17/20000] [Batch 165/192] [D loss: -0.310659] [G loss: 21.880833]\n",
      "[Epoch 17/20000] [Batch 175/192] [D loss: -0.238657] [G loss: 23.847227]\n",
      "[Epoch 17/20000] [Batch 185/192] [D loss: -0.316849] [G loss: 25.053463]\n",
      "[Epoch 18/20000] [Batch 0/192] [D loss: -0.098419] [G loss: 24.104284]\n",
      "[Epoch 18/20000] [Batch 10/192] [D loss: -0.197563] [G loss: 24.857315]\n",
      "[Epoch 18/20000] [Batch 20/192] [D loss: -0.208632] [G loss: 26.766588]\n",
      "[Epoch 18/20000] [Batch 30/192] [D loss: -0.302195] [G loss: 22.011942]\n",
      "[Epoch 18/20000] [Batch 40/192] [D loss: -0.298580] [G loss: 25.388029]\n",
      "[Epoch 18/20000] [Batch 50/192] [D loss: -0.292238] [G loss: 24.419790]\n",
      "[Epoch 18/20000] [Batch 60/192] [D loss: -0.359352] [G loss: 24.761837]\n",
      "[Epoch 18/20000] [Batch 70/192] [D loss: -0.359865] [G loss: 23.527470]\n",
      "[Epoch 18/20000] [Batch 80/192] [D loss: -0.254747] [G loss: 24.809889]\n",
      "[Epoch 18/20000] [Batch 90/192] [D loss: -0.298737] [G loss: 26.960184]\n",
      "[Epoch 18/20000] [Batch 100/192] [D loss: -0.248100] [G loss: 28.009367]\n",
      "[Epoch 18/20000] [Batch 110/192] [D loss: -0.257278] [G loss: 22.153234]\n",
      "[Epoch 18/20000] [Batch 120/192] [D loss: -0.229923] [G loss: 26.480404]\n",
      "[Epoch 18/20000] [Batch 130/192] [D loss: -0.318033] [G loss: 25.412844]\n",
      "[Epoch 18/20000] [Batch 140/192] [D loss: -0.304865] [G loss: 24.967239]\n",
      "[Epoch 18/20000] [Batch 150/192] [D loss: -0.361105] [G loss: 27.640869]\n",
      "[Epoch 18/20000] [Batch 160/192] [D loss: -0.275257] [G loss: 27.467066]\n",
      "[Epoch 18/20000] [Batch 170/192] [D loss: -0.318777] [G loss: 27.106020]\n",
      "[Epoch 18/20000] [Batch 180/192] [D loss: -0.264280] [G loss: 22.448509]\n",
      "[Epoch 18/20000] [Batch 190/192] [D loss: -0.203140] [G loss: 25.804735]\n",
      "[Epoch 19/20000] [Batch 5/192] [D loss: -0.276836] [G loss: 28.776459]\n",
      "[Epoch 19/20000] [Batch 15/192] [D loss: -0.291561] [G loss: 28.071560]\n",
      "[Epoch 19/20000] [Batch 25/192] [D loss: -0.322140] [G loss: 26.996075]\n",
      "[Epoch 19/20000] [Batch 35/192] [D loss: -0.344305] [G loss: 25.804909]\n",
      "[Epoch 19/20000] [Batch 45/192] [D loss: -0.363916] [G loss: 26.167908]\n",
      "[Epoch 19/20000] [Batch 55/192] [D loss: -0.362688] [G loss: 24.269556]\n",
      "[Epoch 19/20000] [Batch 65/192] [D loss: -0.233234] [G loss: 27.622023]\n",
      "[Epoch 19/20000] [Batch 75/192] [D loss: -0.312748] [G loss: 26.292336]\n",
      "[Epoch 19/20000] [Batch 85/192] [D loss: -0.284010] [G loss: 27.784767]\n",
      "[Epoch 19/20000] [Batch 95/192] [D loss: -0.239437] [G loss: 25.803492]\n",
      "[Epoch 19/20000] [Batch 105/192] [D loss: -0.277586] [G loss: 22.737051]\n",
      "[Epoch 19/20000] [Batch 115/192] [D loss: -0.282272] [G loss: 29.086205]\n",
      "[Epoch 19/20000] [Batch 125/192] [D loss: -0.259911] [G loss: 28.150778]\n",
      "[Epoch 19/20000] [Batch 135/192] [D loss: -0.448641] [G loss: 25.135008]\n",
      "[Epoch 19/20000] [Batch 145/192] [D loss: -0.276766] [G loss: 25.642618]\n",
      "[Epoch 19/20000] [Batch 155/192] [D loss: -0.432360] [G loss: 28.745232]\n",
      "[Epoch 19/20000] [Batch 165/192] [D loss: -0.401510] [G loss: 23.831697]\n",
      "[Epoch 19/20000] [Batch 175/192] [D loss: -0.368946] [G loss: 30.253115]\n",
      "[Epoch 19/20000] [Batch 185/192] [D loss: -0.440506] [G loss: 29.084255]\n",
      "[Epoch 20/20000] [Batch 0/192] [D loss: -0.152067] [G loss: 26.775707]\n",
      "[Epoch 20/20000] [Batch 10/192] [D loss: -0.377298] [G loss: 30.653931]\n",
      "[Epoch 20/20000] [Batch 20/192] [D loss: -0.290634] [G loss: 24.482265]\n",
      "[Epoch 20/20000] [Batch 30/192] [D loss: -0.387117] [G loss: 27.368917]\n",
      "[Epoch 20/20000] [Batch 40/192] [D loss: -0.177040] [G loss: 25.638821]\n",
      "[Epoch 20/20000] [Batch 50/192] [D loss: -0.446821] [G loss: 23.709152]\n",
      "[Epoch 20/20000] [Batch 60/192] [D loss: -0.269875] [G loss: 29.853649]\n",
      "[Epoch 20/20000] [Batch 70/192] [D loss: -0.453810] [G loss: 29.903023]\n",
      "[Epoch 20/20000] [Batch 80/192] [D loss: -0.369135] [G loss: 26.333439]\n",
      "[Epoch 20/20000] [Batch 90/192] [D loss: -0.542641] [G loss: 24.865034]\n",
      "[Epoch 20/20000] [Batch 100/192] [D loss: -0.387537] [G loss: 30.667244]\n",
      "[Epoch 20/20000] [Batch 110/192] [D loss: -0.351669] [G loss: 25.358030]\n",
      "[Epoch 20/20000] [Batch 120/192] [D loss: -0.388468] [G loss: 24.111536]\n",
      "[Epoch 20/20000] [Batch 130/192] [D loss: -0.295015] [G loss: 26.616154]\n",
      "[Epoch 20/20000] [Batch 140/192] [D loss: -0.276295] [G loss: 26.270096]\n",
      "[Epoch 20/20000] [Batch 150/192] [D loss: -0.261982] [G loss: 27.711243]\n",
      "[Epoch 20/20000] [Batch 160/192] [D loss: -0.423578] [G loss: 31.683037]\n",
      "[Epoch 20/20000] [Batch 170/192] [D loss: -0.397970] [G loss: 28.892298]\n",
      "[Epoch 20/20000] [Batch 180/192] [D loss: -0.432192] [G loss: 27.899458]\n",
      "[Epoch 20/20000] [Batch 190/192] [D loss: -0.365002] [G loss: 32.354950]\n",
      "[Epoch 21/20000] [Batch 5/192] [D loss: -0.284269] [G loss: 26.661766]\n",
      "[Epoch 21/20000] [Batch 15/192] [D loss: -0.293282] [G loss: 27.463997]\n",
      "[Epoch 21/20000] [Batch 25/192] [D loss: -0.381662] [G loss: 28.148188]\n",
      "[Epoch 21/20000] [Batch 35/192] [D loss: -0.393822] [G loss: 25.163351]\n",
      "[Epoch 21/20000] [Batch 45/192] [D loss: -0.377033] [G loss: 26.186422]\n",
      "[Epoch 21/20000] [Batch 55/192] [D loss: -0.406939] [G loss: 29.497187]\n",
      "[Epoch 21/20000] [Batch 65/192] [D loss: -0.350294] [G loss: 31.437609]\n",
      "[Epoch 21/20000] [Batch 75/192] [D loss: -0.392805] [G loss: 26.505695]\n",
      "[Epoch 21/20000] [Batch 85/192] [D loss: -0.381311] [G loss: 31.836071]\n",
      "[Epoch 21/20000] [Batch 95/192] [D loss: -0.442795] [G loss: 32.960056]\n",
      "[Epoch 21/20000] [Batch 105/192] [D loss: -0.460239] [G loss: 31.415583]\n",
      "[Epoch 21/20000] [Batch 115/192] [D loss: -0.397200] [G loss: 32.583565]\n",
      "[Epoch 21/20000] [Batch 125/192] [D loss: -0.384113] [G loss: 29.692430]\n",
      "[Epoch 21/20000] [Batch 135/192] [D loss: -0.484116] [G loss: 26.016556]\n",
      "[Epoch 21/20000] [Batch 145/192] [D loss: -0.405188] [G loss: 31.720953]\n",
      "[Epoch 21/20000] [Batch 155/192] [D loss: -0.319916] [G loss: 30.260483]\n",
      "[Epoch 21/20000] [Batch 165/192] [D loss: -0.399996] [G loss: 29.339520]\n",
      "[Epoch 21/20000] [Batch 175/192] [D loss: -0.513176] [G loss: 33.461609]\n",
      "[Epoch 21/20000] [Batch 185/192] [D loss: -0.444160] [G loss: 32.713768]\n",
      "[Epoch 22/20000] [Batch 0/192] [D loss: -0.308355] [G loss: 30.955484]\n",
      "[Epoch 22/20000] [Batch 10/192] [D loss: -0.610950] [G loss: 31.166264]\n",
      "[Epoch 22/20000] [Batch 20/192] [D loss: -0.511662] [G loss: 27.890352]\n",
      "[Epoch 22/20000] [Batch 30/192] [D loss: -0.478268] [G loss: 34.372589]\n",
      "[Epoch 22/20000] [Batch 40/192] [D loss: -0.661526] [G loss: 33.053299]\n",
      "[Epoch 22/20000] [Batch 50/192] [D loss: -0.418755] [G loss: 32.930466]\n",
      "[Epoch 22/20000] [Batch 60/192] [D loss: -0.478163] [G loss: 28.316387]\n",
      "[Epoch 22/20000] [Batch 70/192] [D loss: -0.394680] [G loss: 30.759884]\n",
      "[Epoch 22/20000] [Batch 80/192] [D loss: -0.405678] [G loss: 32.633881]\n",
      "[Epoch 22/20000] [Batch 90/192] [D loss: -0.468212] [G loss: 34.153610]\n",
      "[Epoch 22/20000] [Batch 100/192] [D loss: -0.439598] [G loss: 31.316744]\n",
      "[Epoch 22/20000] [Batch 110/192] [D loss: -0.546801] [G loss: 27.144958]\n",
      "[Epoch 22/20000] [Batch 120/192] [D loss: -0.563564] [G loss: 34.029396]\n",
      "[Epoch 22/20000] [Batch 130/192] [D loss: -0.559650] [G loss: 33.998829]\n",
      "[Epoch 22/20000] [Batch 140/192] [D loss: -0.541950] [G loss: 33.691444]\n",
      "[Epoch 22/20000] [Batch 150/192] [D loss: -0.482672] [G loss: 29.524931]\n",
      "[Epoch 22/20000] [Batch 160/192] [D loss: -0.565651] [G loss: 35.038971]\n",
      "[Epoch 22/20000] [Batch 170/192] [D loss: -0.429438] [G loss: 28.881264]\n",
      "[Epoch 22/20000] [Batch 180/192] [D loss: -0.392853] [G loss: 29.615782]\n",
      "[Epoch 22/20000] [Batch 190/192] [D loss: -0.323170] [G loss: 35.155045]\n",
      "[Epoch 23/20000] [Batch 5/192] [D loss: -0.261177] [G loss: 33.481094]\n",
      "[Epoch 23/20000] [Batch 15/192] [D loss: -0.525986] [G loss: 34.254330]\n",
      "[Epoch 23/20000] [Batch 25/192] [D loss: -0.354462] [G loss: 32.901386]\n",
      "[Epoch 23/20000] [Batch 35/192] [D loss: -0.585117] [G loss: 34.650173]\n",
      "[Epoch 23/20000] [Batch 45/192] [D loss: -0.534538] [G loss: 36.080372]\n",
      "[Epoch 23/20000] [Batch 55/192] [D loss: -0.431812] [G loss: 34.305508]\n",
      "[Epoch 23/20000] [Batch 65/192] [D loss: -0.549465] [G loss: 36.205563]\n",
      "[Epoch 23/20000] [Batch 75/192] [D loss: -0.541595] [G loss: 30.515265]\n",
      "[Epoch 23/20000] [Batch 85/192] [D loss: -0.653530] [G loss: 31.465704]\n",
      "[Epoch 23/20000] [Batch 95/192] [D loss: -0.642414] [G loss: 34.575977]\n",
      "[Epoch 23/20000] [Batch 105/192] [D loss: -0.619675] [G loss: 33.117283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/20000] [Batch 115/192] [D loss: -0.616680] [G loss: 36.407173]\n",
      "[Epoch 23/20000] [Batch 125/192] [D loss: -0.434864] [G loss: 33.589176]\n",
      "[Epoch 23/20000] [Batch 135/192] [D loss: -0.614326] [G loss: 38.279881]\n",
      "[Epoch 23/20000] [Batch 145/192] [D loss: -0.388538] [G loss: 31.803207]\n",
      "[Epoch 23/20000] [Batch 155/192] [D loss: -0.499565] [G loss: 31.685579]\n",
      "[Epoch 23/20000] [Batch 165/192] [D loss: -0.433514] [G loss: 35.964622]\n",
      "[Epoch 23/20000] [Batch 175/192] [D loss: -0.523594] [G loss: 39.317886]\n",
      "[Epoch 23/20000] [Batch 185/192] [D loss: -0.416794] [G loss: 32.944157]\n",
      "[Epoch 24/20000] [Batch 0/192] [D loss: -0.253567] [G loss: 36.204845]\n",
      "[Epoch 24/20000] [Batch 10/192] [D loss: -0.654800] [G loss: 31.187334]\n",
      "[Epoch 24/20000] [Batch 20/192] [D loss: -0.582165] [G loss: 35.440910]\n",
      "[Epoch 24/20000] [Batch 30/192] [D loss: -0.501610] [G loss: 37.089844]\n",
      "[Epoch 24/20000] [Batch 40/192] [D loss: -0.551468] [G loss: 32.319363]\n",
      "[Epoch 24/20000] [Batch 50/192] [D loss: -0.577774] [G loss: 33.456779]\n",
      "[Epoch 24/20000] [Batch 60/192] [D loss: -0.514187] [G loss: 39.825081]\n",
      "[Epoch 24/20000] [Batch 70/192] [D loss: -0.753918] [G loss: 35.391964]\n",
      "[Epoch 24/20000] [Batch 80/192] [D loss: -0.507465] [G loss: 40.304646]\n",
      "[Epoch 24/20000] [Batch 90/192] [D loss: -0.630768] [G loss: 38.201523]\n",
      "[Epoch 24/20000] [Batch 100/192] [D loss: -0.528259] [G loss: 36.840729]\n",
      "[Epoch 24/20000] [Batch 110/192] [D loss: -0.540203] [G loss: 34.541306]\n",
      "[Epoch 24/20000] [Batch 120/192] [D loss: -0.596313] [G loss: 38.629730]\n",
      "[Epoch 24/20000] [Batch 130/192] [D loss: -0.636986] [G loss: 40.952698]\n",
      "[Epoch 24/20000] [Batch 140/192] [D loss: -0.645744] [G loss: 41.393143]\n",
      "[Epoch 24/20000] [Batch 150/192] [D loss: -0.822311] [G loss: 34.498158]\n",
      "[Epoch 24/20000] [Batch 160/192] [D loss: -0.718056] [G loss: 44.610294]\n",
      "[Epoch 24/20000] [Batch 170/192] [D loss: -0.627884] [G loss: 36.052994]\n"
     ]
    }
   ],
   "source": [
    "batches_done = 0   # Counter for batches\n",
    "for epoch in range(opt.n_epochs):   # Loop through all epochs\n",
    "    for i, x in enumerate(dataloader): # x is in dataloader (a batch I think). i\n",
    "                                       # is the index of x (number of times critic is trained this epoch)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(x.type(Tensor))   # Variable is a wrapper for the Tensor x was just made into\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()   # Make gradients zero so they don't accumulate\n",
    "\n",
    "        # Sample noise (latent space) to make generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (x.shape[0], opt.latent_dim))))   # Once again Variable wraps the Tensor\n",
    "#         print(type(x))\n",
    "#         print(x.shape)\n",
    "#         print(x[0].shape())\n",
    "#         print(z.shape)\n",
    "\n",
    "        # Generate a batch of images from the latent space sampled\n",
    "        fake_imgs = generator(z)\n",
    "\n",
    "        #print(fake_imgs[0])\n",
    "\n",
    "        # Calculate validity score for real images\n",
    "        real_validity = discriminator(real_imgs)\n",
    "\n",
    "        # Calculate validity score for fake images\n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "\n",
    "        # Calculate gradient penalty\n",
    "        alp = compute_ALP(discriminator, real_imgs.data, fake_imgs.data)\n",
    "        # TODO: figure out why .data is used\n",
    "\n",
    "        # Calculate loss for critic (Adversarial loss)\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + opt.lambda_alp * alp\n",
    "\n",
    "        d_loss.backward()   # Do back propagation \n",
    "        optimizer_D.step()   # Update parameters based on gradients for individuals\n",
    "\n",
    "        optimizer_G.zero_grad()   # Resets gradients for generator to be zero to avoid accumulation\n",
    "\n",
    "        # Train the generator every n_critic steps\n",
    "        if i % opt.n_critic == 0:\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            # Generate a batch of images\n",
    "            fake_imgs = generator(z)\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            # Train on fake images\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "\n",
    "            # ----------------------------\n",
    "            # Save stuff when time is right\n",
    "            # ----------------------------\n",
    "            if batches_done % 10 == 0:\n",
    "                print(\n",
    "                    \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                    % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "                )\n",
    "\n",
    "            if batches_done % sample_interval == 0:\n",
    "                save_image(fake_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "            batches_done += n_critic\n",
    "    if epoch % 10 == 0:\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (300000, opt.latent_dim))))\n",
    "        fake_data = generator(z)\n",
    "        np.save('./gen_data_alp/{num_batches}.npy'.format(num_batches=batches_done), fake_data.cpu().detach().numpy())\n",
    "\n",
    "torch.save(generator.model, opt.configDir + \"/generatorModel.pt\")\n",
    "torch.save(discriminator.model, opt.configDir + \"/discriminatorModel.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GANS)",
   "language": "python",
   "name": "gans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
